{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook for gathering celebrity chef recipe data via Food Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import unicodedata\n",
    "import re\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    '''\n",
    "    url: str, webpage address to scrape\n",
    "    page: str, webpage DOM\n",
    "    '''\n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    return page\n",
    "\n",
    "def get_soup(webpage):\n",
    "    '''\n",
    "    webpage: str, page to soup\n",
    "    soup: bs4.BeautifulSoup object, souped page\n",
    "    '''\n",
    "    soup = BeautifulSoup(webpage)\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1. Assemble links to chef pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chefs and the numbers of pages of recipes they have on the Food Network website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chefs_and_pages = {\n",
    "    'Valerie Bertinelli': 11,\n",
    "    'Ina Garten': 110,\n",
    "    'Ree Drummond': 77,\n",
    "    'Giada De Laurentiis': 173,\n",
    "    'Trisha Yearwood': 33,\n",
    "    'Guy Fieri': 85, \n",
    "    'Robert Irvine': 99,\n",
    "    'Alton Brown': 76,\n",
    "    'Bobby Flay': 198,\n",
    "    'Duff Goldman': 5, \n",
    "    'Sunny Anderson': 52,\n",
    "    'Marcela Valladolid': 33\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to change name format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_chef_name_format(chef):\n",
    "    a = chef.lower().split()\n",
    "    name = ''\n",
    "    for n in range(len(a)):\n",
    "        name += a[n] + '-'\n",
    "    name = name[:-1]\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_chef_links(chefs_and_pages):\n",
    "    links = {}\n",
    "    for chef, pages in chefs_and_pages.items():\n",
    "        name = change_chef_name_format(chef)\n",
    "        for i in range(1, pages+1):\n",
    "            link = 'http://www.foodnetwork.com/chefs/' + name + '/recipes.mostpopular.page-' \\\n",
    "            + str(i) + '.html'\n",
    "            if chef not in links:\n",
    "                links[chef] = [link]\n",
    "            else:\n",
    "                links[chef].append(link)  \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chef_page_links = get_chef_links(chefs_and_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. Get recipe page links from chef pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_recipe_links(chef_page_links):\n",
    "    recipe_links = {}\n",
    "    \n",
    "    for chef, pages in chef_page_links.items():\n",
    "        soups = []\n",
    "        for link in tqdm(pages):\n",
    "            soup = get_soup(get_page(link))\n",
    "            soups.append(soup)\n",
    "            time.sleep(.002)\n",
    "        recipe_links[chef] = soups\n",
    "        \n",
    "    for chef, pages in recipe_links.items():\n",
    "        name = change_chef_name_format(chef)\n",
    "        recipes = []\n",
    "        for page in pages:\n",
    "            page = page.find_all('h6')\n",
    "            for line in page:\n",
    "                line = str(line).split()\n",
    "                if len(line) > 1 and '/recipes/' + name + '/' in line[1]:\n",
    "                    recipes.append('http://www.foodnetwork.com' + line[1][6:-1])\n",
    "        recipe_links[chef] = recipes\n",
    "    \n",
    "    return recipe_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recipe_links = get_recipe_links(chef_page_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pickling\n",
    "with open('recipe_links.pkl', 'w') as picklefile:\n",
    "    pickle.dump(recipe_links, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3. Get recipe info from recipe pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#unpickling\n",
    "with open('recipe_links.pkl', 'r') as picklefile: \n",
    "    recipe_links = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "recipe_info_keys = ['total_time','cook_time','ingredients','difficulty_level',\n",
    "        'yield','inactive_time','prep_time','directions','categories','page_link','img_link']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_recipe_info(recipe_links=recipe_links, keys=recipe_info_keys):\n",
    "    chefs_and_recipes = {}\n",
    "    for chef, links in recipe_links.items():\n",
    "        print chef\n",
    "        recipes = {}\n",
    "        for link in tqdm(links):\n",
    "            soup = get_soup(get_page(link))\n",
    "            info = {'page_link': link}\n",
    "            \n",
    "            img = soup.find_all('div', {'class': 'col12 pic collapsed'})\n",
    "            for line in str(img).split('\\n'):\n",
    "                if 'src=' in line:\n",
    "                    info['img_link'] = line.split('src=\"')[1].split('\"')[0]\n",
    "             \n",
    "            times = soup.find('div', {'class': 'cooking-times'})  \n",
    "            for line in str(times).split('\\n'):\n",
    "                if 'Total' in line:\n",
    "                    info['total_time'] = line.split('<dd class=\"total\">')[1].split('<')[0]\n",
    "                if 'Prep' in line:\n",
    "                    info['prep_time'] = line.split('<dd>')[1].split('<')[0]\n",
    "                if 'Inactive' in line:\n",
    "                    info['inactive_time'] = line.split('<dd>')[1].split('<')[0]\n",
    "                if 'Cook' in line:\n",
    "                    info['cook_time'] = line.split('<dd>')[1].split('<')[0]\n",
    "\n",
    "            yield_and_level = soup.find('div', {'class': 'difficulty'})\n",
    "            yield_and_level_split = str(yield_and_level).split('\\n')\n",
    "            for i, line in enumerate(yield_and_level_split):\n",
    "                if 'Yield' in line:\n",
    "                    try:\n",
    "                        info['yield'] = yield_and_level_split[i+1].split('<dd>')[1].split(\n",
    "                            '<')[0]\n",
    "                    except:\n",
    "                        pass\n",
    "                if 'Level' in line:\n",
    "                    info['difficulty_level'] = yield_and_level_split[i+1].split('<dd>')[\n",
    "                        1].split('<')[0]\n",
    "\n",
    "            ingredients = soup.find_all('div', {'class': 'col8 ingredients responsive'})\n",
    "            ingredient_list = []\n",
    "            for line in str(ingredients).split('\\n'):\n",
    "                if 'itemprop=\"ingredients\"' in line:   \n",
    "                    ingredient_list.append(line.split('\"ingredients\">')[1].split('<')[0])\n",
    "            info['ingredients'] = ingredient_list\n",
    "\n",
    "            directions = soup.find_all('div', {'class': 'col10 directions'})\n",
    "            if directions != []:\n",
    "                for line in str([line.text for line in directions]).split('\\\\n'):\n",
    "                    if len(line) > 200:\n",
    "                        info['directions'] = line\n",
    "            \n",
    "            categories = soup.find_all('div', {'class': 'categories'})\n",
    "            categories_list = []\n",
    "            for line in str(categories).split('\\n'):\n",
    "                if 'href=\"/topics/' in line:\n",
    "                    categories_list.append(line.split('.html\">')[1].split('<')[0])\n",
    "            info['categories'] = categories_list\n",
    "        \n",
    "            title = soup.find('div', {'class': 'tier-3 title'})\n",
    "            for line in str(title).split('\\n'):\n",
    "                if 'itemprop=\"name\"' in line:  \n",
    "                    title = line.split('\"name\">')[1].split('<')[0]\n",
    "            info['title'] = title\n",
    "            \n",
    "            ## If wanting NaNs, include these lines:\n",
    "            #             entries = info.keys()\n",
    "            #             na = str(np.nan)\n",
    "            #             for key in recipe_info_keys:\n",
    "            #                 if key not in entries:\n",
    "            #                     info[key] = na\n",
    "            #                 if info[key] == []:\n",
    "            #                     info[key] = [na]\n",
    "                    \n",
    "            recipes[title] = info \n",
    "            time.sleep(.002)\n",
    "            \n",
    "        chefs_and_recipes[chef] = recipes\n",
    "    return chefs_and_recipes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going in segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fieri0 = {'Guy Fieri': recipe_links['Guy Fieri'][:400]}\n",
    "fieri1 = {'Guy Fieri': recipe_links['Guy Fieri'][400:]}\n",
    "fieri_info_0 = get_recipe_info(fieri0)\n",
    "fieri_info_1 = get_recipe_info(fieri1)\n",
    "###pickling!#####################################################\n",
    "##### -- \"but first you've got to barrel your scrapes\" -- ########\n",
    "with open('fieri_betterinfo_0.pkl', 'w') as picklefile:         \n",
    "    pickle.dump(fieri_info_0, picklefile)                 \n",
    "with open('fieri_betterinfo_1.pkl', 'w') as picklefile:     \n",
    "    pickle.dump(fieri_info_1, picklefile)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yearwood ={'Trisha Yearwood': recipe_links['Trisha Yearwood']}\n",
    "yearwood_info = get_recipe_info(yearwood)\n",
    "###pickling!#####################################################\n",
    "##### -- \"but first you've got to barrel your scrapes\" -- ########\n",
    "with open('yearwood_betterinfo.pkl', 'w') as picklefile:      \n",
    "    pickle.dump(yearwood_info, picklefile)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "giada0 = {'Giada De Laurentiis': recipe_links['Giada De Laurentiis'][:400]}\n",
    "giada1 = {'Giada De Laurentiis': recipe_links['Giada De Laurentiis'][400:850]}\n",
    "giada2 = {'Giada De Laurentiis': recipe_links['Giada De Laurentiis'][850:1300]}\n",
    "giada3 = {'Giada De Laurentiis': recipe_links['Giada De Laurentiis'][1300:]}\n",
    "giada_info_0 = get_recipe_info(giada0)\n",
    "giada_info_1 = get_recipe_info(giada1)\n",
    "giada_info_2 = get_recipe_info(giada2)\n",
    "giada_info_3 = get_recipe_info(giada3)\n",
    "###pickling!#####################################################\n",
    "##### -- \"but first you've got to barrel your scrapes\" -- ########\n",
    "with open('giada_betterinfo_0.pkl', 'w') as picklefile:\n",
    "    pickle.dump(giada_info_0, picklefile)\n",
    "with open('giada_betterinfo_1.pkl', 'w') as picklefile:\n",
    "    pickle.dump(giada_info_1, picklefile)\n",
    "with open('giada_betterinfo_2.pkl', 'w') as picklefile:\n",
    "    pickle.dump(giada_info_2, picklefile)\n",
    "with open('giada_betterinfo_3.pkl', 'w') as picklefile:\n",
    "    pickle.dump(giada_info_3, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alton0 = {'Alton Brown': recipe_links['Alton Brown'][:400]}\n",
    "alton1 = {'Alton Brown': recipe_links['Alton Brown'][400:]}\n",
    "alton_info_0 = get_recipe_info(alton0)\n",
    "alton_info_1 = get_recipe_info(alton1)\n",
    "###pickling!##################################################\n",
    "##### -- \"but first you've got to barrel your scrapes\" -- #####\n",
    "with open('alton_betterinfo_0.pkl', 'w') as picklefile:\n",
    "    pickle.dump(alton_info_0, picklefile)\n",
    "with open('alton_betterinfo_1.pkl', 'w') as picklefile:\n",
    "    pickle.dump(alton_info_1, picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bertinelli = {'Valerie Bertinelli': recipe_links['Valerie Bertinelli']}\n",
    "bertinelli_info = get_recipe_info(bertinelli)\n",
    "###pickling!#####################################################\n",
    "##### -- \"but first you've got to barrel your scrapes\" -- ########\n",
    "with open('bertinelli_betterinfo.pkl', 'w') as picklefile:        \n",
    "    pickle.dump(bertinelli_info, picklefile)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valladolid = {'Marcela Valladolid': recipe_links['Marcela Valladolid']}\n",
    "valladolid_info = get_recipe_info(valladolid)\n",
    "###pickling!#####################################################\n",
    "##### -- \"but first you've got to barrel your scrapes\" -- ########\n",
    "with open('valladolid_betterinfo.pkl', 'w') as picklefile:  \n",
    "    pickle.dump(valladolid_info, picklefile)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "garten0 = {'Ina Garten': recipe_links['Ina Garten'][:400]}\n",
    "garten1 = {'Ina Garten': recipe_links['Ina Garten'][400:800]}\n",
    "garten2 = {'Ina Garten': recipe_links['Ina Garten'][800:]}\n",
    "garten_info_0 = get_recipe_info(garten0)\n",
    "garten_info_1 = get_recipe_info(garten1)\n",
    "garten_info_2 = get_recipe_info(garten2)\n",
    "###pickling!#####################################################\n",
    "##### -- \"but first you've got to barrel your scrapes\" -- ########\n",
    "with open('garten_betterinfo_0.pkl', 'w') as picklefile:        \n",
    "    pickle.dump(garten_info_0, picklefile)            \n",
    "with open('garten_betterinfo_1.pkl', 'w') as picklefile:   \n",
    "    pickle.dump(garten_info_1, picklefile)             \n",
    "with open('garten_betterinfo_2.pkl', 'w') as picklefile:   \n",
    "    pickle.dump(garten_info_2, picklefile)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drummond0 = {'Ree Drummond': recipe_links['Ree Drummond'][:400]}\n",
    "drummond1 = {'Ree Drummond': recipe_links['Ree Drummond'][400:]}\n",
    "drummond_info_0 = get_recipe_info(drummond0)\n",
    "drummond_info_1 = get_recipe_info(drummond1)\n",
    "###pickling!#####################################################\n",
    "##### -- \"but first you've got to barrel your scrapes\" -- ########\n",
    "with open('drummond_betterinfo_0.pkl', 'w') as picklefile:     \n",
    "    pickle.dump(drummond_info_0, picklefile)              \n",
    "with open('drummond_betterinfo_1.pkl', 'w') as picklefile:     \n",
    "    pickle.dump(drummond_info_1, picklefile)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flay0 = {'Bobby Flay': recipe_links['Bobby Flay'][:500]}\n",
    "flay1 = {'Bobby Flay': recipe_links['Bobby Flay'][500:1000]}\n",
    "flay2 = {'Bobby Flay': recipe_links['Bobby Flay'][1000:1500]}\n",
    "flay3 = {'Bobby Flay': recipe_links['Bobby Flay'][1500:]}\n",
    "flay_info_0 = get_recipe_info(flay0)\n",
    "flay_info_1 = get_recipe_info(flay1)\n",
    "flay_info_2 = get_recipe_info(flay2)\n",
    "flay_info_3 = get_recipe_info(flay3)\n",
    "###pickling!#####################################################\n",
    "##### -- \"but first you've got to barrel your scrapes\" -- ########\n",
    "with open('flay_betterinfo_0.pkl', 'w') as picklefile:        \n",
    "    pickle.dump(flay_info_0, picklefile)               \n",
    "with open('flay_betterinfo_1.pkl', 'w') as picklefile:       \n",
    "    pickle.dump(flay_info_1, picklefile)            \n",
    "with open('flay_betterinfo_2.pkl', 'w') as picklefile:     \n",
    "    pickle.dump(flay_info_2, picklefile)  \n",
    "with open('flay_betterinfo_3.pkl', 'w') as picklefile:     \n",
    "    pickle.dump(flay_info_3, picklefile) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "irvine0 = {'Robert Irvine': recipe_links['Robert Irvine'][:450]}\n",
    "irvine1 = {'Robert Irvine': recipe_links['Robert Irvine'][450:]}\n",
    "irvine_info_0 = get_recipe_info(irvine0)\n",
    "irvine_info_1 = get_recipe_info(irvine1)\n",
    "###pickling!#####################################################\n",
    "##### -- \"but first you've got to barrel your scrapes\" -- ########\n",
    "with open('irvine_betterinfo_0.pkl', 'w') as picklefile:        \n",
    "    pickle.dump(irvine_info_0, picklefile)             \n",
    "with open('irvine_betterinfo_1.pkl', 'w') as picklefile:  \n",
    "    pickle.dump(irvine_info_1, picklefile)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sunny = {'Sunny Anderson': recipe_links['Sunny Anderson']}\n",
    "sunny_info = get_recipe_info(sunny)\n",
    "###pickling!#####################################################\n",
    "##### -- \"but first you've got to barrel your scrapes\" -- ########\n",
    "with open('sunny_betterinfo.pkl', 'w') as picklefile:         \n",
    "    pickle.dump(sunny_info, picklefile)             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "duff = {'Duff Goldman': recipe_links['Duff Goldman']}\n",
    "duff_info = get_recipe_info(duff)\n",
    "###pickling!#####################################################\n",
    "##### -- \"but first you've got to barrel your scrapes\" -- ########\n",
    "with open('duff_betterinfo.pkl', 'w') as picklefile:    \n",
    "    pickle.dump(duff_info, picklefile)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
