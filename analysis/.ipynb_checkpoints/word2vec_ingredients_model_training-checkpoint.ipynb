{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A notebook for training a word2vec model on a corpus of recipes' ingredients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "#import enchant\n",
    "#from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient()\n",
    "chefs = client.chefs_db\n",
    "celebrity_recipes = client.chefs_db.celebrity_recipes\n",
    "yummly_recipes = client.chefs_db.yummly_recipes\n",
    "yummly_recipes2 = client.chefs_db.yummly_recipes2\n",
    "yummly_recipes3 = client.chefs_db.yummly_recipes3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'_id': ObjectId('56f0a831fdb32c06bd913f81'),\n",
       " u'celebrity_recipe': u'Vanilla Frosting',\n",
       " u'chef': u'Giada De Laurentiis',\n",
       " u'flavors': {u'bitter': 0.16666666666666666,\n",
       "  u'meaty': 0.8333333333333334,\n",
       "  u'piquant': 0.0,\n",
       "  u'salty': 0.16666666666666666,\n",
       "  u'sour': 0.16666666666666666,\n",
       "  u'sweet': 0.8333333333333334},\n",
       " u'yummly_ingredients': [u'vegetable shortening',\n",
       "  u'powdered sugar',\n",
       "  u'vanilla extract',\n",
       "  u'milk'],\n",
       " u'yummly_recipe': u'Vanilla Frosting'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yummly_recipes2.find_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u' butter', u'powdered sugar', u'milk', u'vanilla extract']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celebrity_recipes.find_one()['ingredients']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1. Corpus-building\n",
    "####collecting ingredients from:\n",
    "(a) celebrity recipes\n",
    "\n",
    "(b) yummly recipes (1, 2, eventually 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "specs = ['dash', 'pinch', 'teaspoons', 'tablespoons', 'cup', 'scoop', 'pound', 'ounce', 'oz', \n",
    "         'quart', 'pint', 'gallon', 'milliliter', 'ml', 'liter', 'small', 'medium', 'large', \n",
    "         'freshly', 'ground', 'piece', 'clove', 'boneless', 'cube', 'dice', 'finely', \n",
    "         'grated', 'to', 'inch', 'each', 'whole', 'about', 'as', 'thawed', 'by', 'all', 'a',\n",
    "         'chopped', 'crushed', 'plus', 'minus', 'such', 'the', 'an', 'slice', 'approximately',\n",
    "         'and', 'or', 'weight', 'of', 'recipe', 'basic', 'slab', 'stick', 'pure', 'melt', \n",
    "         'melted', 'minute', 'add', 'heat', 'place', 'set', 'top', 'bring', 'bowl', 'hour', \n",
    "         'preheat', 'together', 'serve', 'stir', 'serving', 'side', 'valerie', 'bertinelli', \n",
    "         'dish', 'time', 'italian', 'recipe', 'childhood', 'provided', 'reserved', 'right', \n",
    "         'courtesy', 'american', 'degree', 'cook', 'pan', 'mix', 'mixture', 'season', 'food', \n",
    "         'pour', 'use', 'water', 'high', 'using', 'remove', 'let', 'taste', 'duff', 'like', \n",
    "         'oven', 'sheet', 'onto', 'also', 'boil', 'day', 'sit', 'room', 'cooled', 'cover',\n",
    "         'jar', 'least', 'enough', 'jarcombine', 'special', 'pounds', 'head', 'extract', \n",
    "         'packed', 'dark', 'light', 'stick', 'sticks', '-inch-thick', 'sprig', 'ounces'\n",
    "         'package', 'dried', 'dry', 'wet', 'coarse', 'fine', 'ground', 'precooked', 'perfect', \n",
    "         'leaf', 'leaves', 'flat-leaf', 'seed', 'fillet', 'recommended', 'squeezed', 'juice',\n",
    "         'powdered', 'condensed', 'concentrate', 'extra-large', 'thin', 'peeled', 'andor',\n",
    "         'thigh', 'breast', 'dale', 'free', 'enhancer', 'coke', 'casserole', 'dale', 'hp', \n",
    "         'pot', 'weed', 'santo', 'fresh', 'yardlong', 'inner', 'blade', 'outer', 'bucco', \n",
    "         'low', 'bottom', 'top', 'navy', 'eye', 'umms', 'veri', 'flat', 'tenderized', \n",
    "         'tenderizer', 'landis', 'marinating', 'reduced', 'open', 'close', 'excess', 'le', \n",
    "         'isolate', 'lower', 'leftover', 'complete', 'pace', 'pit', 'stacys', 'refined', 'tip', \n",
    "         'au', 'jus', 'ey', 'vay', 'oigatsuo', 'topside', 'hsing', 'pace', 'par', 'yet', \n",
    "         'nielsen', 'flap', 'crosse', 'di', 'grilling', 'braising', 'vital', 'columbia', \n",
    "         'left', 'right', 'bing', 'blade', 'crest', 'blue']\n",
    "stops = set(specs + stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_ingredients(ingredients):\n",
    "    #d = enchant.Dict(\"en_US\")\n",
    "    ingredients = ' '.join([re.compile('[^a-zA-Z]').sub(' ', i).lower() for i in ingredients])\n",
    "    ingredients = [WordNetLemmatizer().lemmatize(word) for word in ingredients.split() \n",
    "                   if len(word) > 1]\n",
    "                   #and d.check(word) <-takes way long\n",
    "                   #and WordNetLemmatizer().lemmatize(word) not in stops] <-cuts corp too much\n",
    "    return ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the model's performance may have marginally improved without foreign words, checking whether every word was in English proved untenable given the time constraints of this project. This was all the more true of using NLTK's `pos_tag` method to gauge part of speech for every word, in order to, say, keep only nouns, proper nouns, and adjectives.\n",
    "\n",
    "Additionally, removing stopwords worsened model performance regardless of the number of stopwords. Therefore, stopwords were ultimately not used in putting together the final corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next collection\n",
      "next collection"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "next collection"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "errorcount = 0\n",
    "\n",
    "cursor = celebrity_recipes.find()\n",
    "for recipe in tqdm(cursor):\n",
    "    try:\n",
    "        ingredients = clean_ingredients(recipe['ingredients'])\n",
    "        corpus.append(ingredients)\n",
    "    except:\n",
    "        errorcount += 1\n",
    "        print 'recipes w/o ingredients:', errorcount\n",
    "\n",
    "for coll in [yummly_recipes, yummly_recipes2, yummly_recipes3]:\n",
    "    print 'next collection'\n",
    "    cursor = coll.find()\n",
    "    for recipe in tqdm(cursor):\n",
    "        try:\n",
    "            ingredients = clean_ingredients(recipe['yummly_ingredients'])\n",
    "            corpus.append(ingredients)\n",
    "        except:\n",
    "            errorcount += 1\n",
    "            print 'recipes w/o ingredients:', errorcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3022453"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Each document in the corpus consists of a list of ingredients called for in a particular recipe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. Train w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "model = Word2Vec(tqdm(corpus), min_count=1)\n",
    "with open('word2vec_ingredients_take_4.pkl', 'w') as picklefile:\n",
    "    pickle.dump(model, picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus: a preliminary example using the trained w2v:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('word2vec_ingredients.pkl', 'r') as picklefile: \n",
    "    w2v_trained = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96788931612928597"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws1=[u' butter', u'powdered sugar', u'milk', u'vanilla extract']\n",
    "ws2=[u'vegetable shortening', u'powdered sugar', u'vanilla extract', u'milk']\n",
    "w2v_trained.n_similarity(ws1, ws2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
